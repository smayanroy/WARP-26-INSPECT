{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e0ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958dac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f943fd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from C:\\Users\\SMAYA\\_netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mb25cs1065\u001b[0m (\u001b[33mb25cs1065-indian-institute-of-technology-jodhpur\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\SMAYA\\OneDrive\\Desktop\\python\\wandb\\run-20260205_190534-32zht66j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch/runs/32zht66j' target=\"_blank\">elated-fog-16</a></strong> to <a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch' target=\"_blank\">https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch/runs/32zht66j' target=\"_blank\">https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch/runs/32zht66j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"CNN on MNIST pytorch\", save_code=True,\n",
    "           config={\n",
    "               \"batch_size\": 128,\n",
    "               \"epochs\": 20,\n",
    "               \"optimizer\": \"Adam\",\n",
    "               \"loss function\": \"CrossEntropyLoss\",\n",
    "           }\n",
    "           )\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf7325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14930b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: torch.Size([60000, 28, 28])\n",
      "training labels shape: torch.Size([60000])\n",
      "testing data shape: torch.Size([10000, 28, 28])\n",
      "testing labels shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "#Prepare the MNIST dataset in this cell\n",
    "''\n",
    "train_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.RandomRotation(degrees=10),\n",
    "                                transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.7, 1.3)),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "''\n",
    "test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
    "print(f\"training data shape: {trainset.data.shape}\")\n",
    "print(f\"training labels shape: {trainset.targets.shape}\")\n",
    "print(f\"testing data shape: {testset.data.shape}\")\n",
    "print(f\"testing labels shape: {testset.targets.shape}\")\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e5b277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neural_network(\n",
       "  (conv1): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (normalization1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (normalization2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (normalization3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=10, bias=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neural_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Neural_network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 12, 3, padding=1)#output dim = 14x14x8\n",
    "        self.conv2 = nn.Conv2d(12, 24, 3)#output dim = 7x7x16\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.normalization1 = nn.BatchNorm2d(12)\n",
    "        self.normalization2 = nn.BatchNorm2d(24)\n",
    "        self.normalization3 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(24, 32, 3)#output dim = 3x3x20\n",
    "        self.fc1 = nn.Linear(4*4*32, 16)\n",
    "        self.fc2 = nn.Linear(16, 10)\n",
    "        #self.fc3 = nn.Linear(16, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    def forward(self,x):\n",
    "        x= self.pool(F.leaky_relu(self.normalization1(self.conv1(x))))\n",
    "        x= self.pool(F.leaky_relu(self.normalization2(self.conv2(x))))\n",
    "        x= F.leaky_relu(self.normalization3(self.conv3(x)))\n",
    "        #print(f\"Shape after conv3 and pool: {x.shape}\")\n",
    "        x= x.view(-1, 4*4*32)# understand the math of this line\n",
    "        x= F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Neural_network()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "725004cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the model: 18194\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(f\"Total number of parameters in the model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b865b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)#does net.parameters just make a vector of all the parameters in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da1e64f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000023EF08B60D0>\n"
     ]
    }
   ],
   "source": [
    "print(f\"{trainloader}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d7a084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nepochs = 20\\nfor epoch in range(epochs):\\n    dynamic_loss = 0.0\\n    for i,data in enumerate(trainloader):\\n        inputs, labels = data[0].to(device), data[1].to(device)\\n        optimizer.zero_grad()\\n        outputs = net(inputs)\\n        loss = loss_function(outputs, F.one_hot(labels, num_classes=10).float())\\n        loss.backward()\\n        optimizer.step()\\n        dynamic_loss += loss\\n        if i%200 ==99:\\n            print(f\"epoch {epoch+1}, batch {i+1}, loss: {dynamic_loss/400}\")\\n            dynamic_loss = 0.0 \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#have to decide how to train the model in this cell ie epochs, batch size, backpropagation etc.\n",
    "'''\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    dynamic_loss = 0.0\n",
    "    for i,data in enumerate(trainloader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_function(outputs, F.one_hot(labels, num_classes=10).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        dynamic_loss += loss\n",
    "        if i%200 ==99:\n",
    "            print(f\"epoch {epoch+1}, batch {i+1}, loss: {dynamic_loss/400}\")\n",
    "            dynamic_loss = 0.0 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8780841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom tqdm import tqdm\\npbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}\")\\nfor batch_idx, (data,target) in enumerate(pbar):\\n    inputs, labels = data[0].to(device), data[1].to(device)\\n    optimizer.zero_grad()\\n    outputs = net(inputs)\\n    loss = loss_function(outputs, F.one_hot(labels, num_classes=10).float())\\n    loss.backward()\\n    optimizer.step()\\n    pbar.set_postfix({\\'loss\\': loss.item()})\\n    wandb.log({\"train_loss\": loss.item()})\\n    dynamic_loss += loss\\n    if i%200 ==99:\\n        print(f\"epoch {epoch+1}, batch {i+1}, loss: {dynamic_loss/400}\")\\n        dynamic_loss = 0.0 \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from tqdm import tqdm\n",
    "pbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}\")\n",
    "for batch_idx, (data,target) in enumerate(pbar):\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = loss_function(outputs, F.one_hot(labels, num_classes=10).float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_postfix({'loss': loss.item()})\n",
    "    wandb.log({\"train_loss\": loss.item()})\n",
    "    dynamic_loss += loss\n",
    "    if i%200 ==99:\n",
    "        print(f\"epoch {epoch+1}, batch {i+1}, loss: {dynamic_loss/400}\")\n",
    "        dynamic_loss = 0.0 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1077baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, device, train_loader, optimizer, epoch):\n",
    "    net.train()\n",
    "    # tqdm creates the progress bar\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    train_correct =0\n",
    "    train_total =0\n",
    "    for i, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(data)\n",
    "        loss = loss_function(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, train_predicted = torch.max(outputs, 1)\n",
    "        train_total += len(target)\n",
    "        train_correct += (train_predicted == target).sum().item()\n",
    "        # Update the bar with the latest loss\n",
    "        pbar.set_description(desc=f'Epoch {epoch} Loss={loss.item():.4f}')\n",
    "        \n",
    "        # Log training loss to wandb\n",
    "        wandb.log({\"train_loss\": loss.item(), \"train_accuracy\": 100.0 * train_correct / train_total})\n",
    "        pbar.set_postfix({'loss': loss.item(), 'train_accuracy': 100.0 * train_correct / train_total})\n",
    "\n",
    "def test(net, device, testloader):\n",
    "    net.eval()\n",
    "    correct_labels = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            test_loss += loss_function(outputs, labels).item()\n",
    "            #print(f\"outputs.data = {outputs.data}\")\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            #if (predicted == labels).sum().item():\n",
    "            correct_labels += (predicted == labels).sum().item()\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    accuracy = 100.0 * correct_labels / len(testloader.dataset)\n",
    "    wandb.log({\"test_accuracy\": accuracy, \"test_loss\": test_loss})\n",
    "    print(f\"\\n Test Set: Accuracy: {accuracy:.4f}%, test_loss: {test_loss} \\n\")\n",
    "# Log test results to wandb\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d71c37e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'correct_labels = 0\\ntotal_labels = 0\\nwith torch.no_grad():\\n    for data in testloader:\\n        images, labels = data[0].to(device), data[1].to(device)\\n        outputs = net(images)\\n        probs = F.softmax(outputs, dim =1)\\n        #print(f\"outputs.data = {outputs.data}\")\\n        _, predicted = torch.max(probs, 1)\\n        total_labels += labels.size(0)\\n        #if (predicted == labels).sum().item():\\n        correct_labels += (predicted == labels).sum().item()\\n        wandb.log({\"test_accuracy\": 100*correct_labels/total_labels})\\nprint(f\"accuracy of the network on the 10000 test images: {100*correct_labels/total_labels} %\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''correct_labels = 0\n",
    "total_labels = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        probs = F.softmax(outputs, dim =1)\n",
    "        #print(f\"outputs.data = {outputs.data}\")\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "        total_labels += labels.size(0)\n",
    "        #if (predicted == labels).sum().item():\n",
    "        correct_labels += (predicted == labels).sum().item()\n",
    "        wandb.log({\"test_accuracy\": 100*correct_labels/total_labels})\n",
    "print(f\"accuracy of the network on the 10000 test images: {100*correct_labels/total_labels} %\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d90527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss=0.4172: 100%|██████████| 469/469 [02:00<00:00,  3.90it/s, loss=0.417, train_accuracy=72.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 97.1100%, test_loss: 0.0008295545760542154 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss=0.2389: 100%|██████████| 469/469 [14:37<00:00,  1.87s/it, loss=0.239, train_accuracy=90.4]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 97.0600%, test_loss: 0.0007285563215147704 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss=0.1658: 100%|██████████| 469/469 [00:33<00:00, 14.07it/s, loss=0.166, train_accuracy=92.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 97.2800%, test_loss: 0.0006223651727195829 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss=0.1888: 100%|██████████| 469/469 [00:53<00:00,  8.77it/s, loss=0.189, train_accuracy=94]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.0600%, test_loss: 0.0004596131012949627 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss=0.1932: 100%|██████████| 469/469 [01:17<00:00,  6.05it/s, loss=0.193, train_accuracy=94.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.4600%, test_loss: 0.0003786530995887006 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss=0.0669: 100%|██████████| 469/469 [01:18<00:00,  5.96it/s, loss=0.0669, train_accuracy=95]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.5700%, test_loss: 0.00032161601687257646 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss=0.1185: 100%|██████████| 469/469 [02:06<00:00,  3.70it/s, loss=0.119, train_accuracy=95.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.6700%, test_loss: 0.0003000000886851922 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss=0.2862: 100%|██████████| 469/469 [01:32<00:00,  5.08it/s, loss=0.286, train_accuracy=95.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.8800%, test_loss: 0.00027982439685147255 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss=0.1249: 100%|██████████| 469/469 [02:04<00:00,  3.77it/s, loss=0.125, train_accuracy=95.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.6200%, test_loss: 0.00032700320551521143 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss=0.0841: 100%|██████████| 469/469 [01:59<00:00,  3.93it/s, loss=0.0841, train_accuracy=95.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.7300%, test_loss: 0.00029795467034564354 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss=0.1264: 100%|██████████| 469/469 [01:58<00:00,  3.95it/s, loss=0.126, train_accuracy=95.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.8800%, test_loss: 0.0002849913059733808 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss=0.1277: 100%|██████████| 469/469 [01:58<00:00,  3.97it/s, loss=0.128, train_accuracy=96.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.5600%, test_loss: 0.00032570452569634655 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss=0.1768: 100%|██████████| 469/469 [01:51<00:00,  4.22it/s, loss=0.177, train_accuracy=96.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.9500%, test_loss: 0.0002497715447192604 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss=0.2030: 100%|██████████| 469/469 [01:48<00:00,  4.33it/s, loss=0.203, train_accuracy=96.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.9100%, test_loss: 0.00023391506910702448 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss=0.2424: 100%|██████████| 469/469 [02:03<00:00,  3.78it/s, loss=0.242, train_accuracy=96.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.8900%, test_loss: 0.00027147889290499736 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss=0.1461: 100%|██████████| 469/469 [01:12<00:00,  6.46it/s, loss=0.146, train_accuracy=96.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.1600%, test_loss: 0.00022495543458207976 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss=0.0613: 100%|██████████| 469/469 [01:07<00:00,  6.91it/s, loss=0.0613, train_accuracy=96.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.8700%, test_loss: 0.0002667698753393779 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss=0.0529: 100%|██████████| 469/469 [01:22<00:00,  5.69it/s, loss=0.0529, train_accuracy=96.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.9900%, test_loss: 0.00023505081380644698 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss=0.0849: 100%|██████████| 469/469 [01:03<00:00,  7.40it/s, loss=0.0849, train_accuracy=96.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.7100%, test_loss: 0.00027698351168219235 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss=0.1187: 100%|██████████| 469/469 [03:32<00:00,  2.21it/s, loss=0.119, train_accuracy=96.6]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.8200%, test_loss: 0.00026225781759640085 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁▁▂▄▆▆▆▇▆▇▇▆▇▇▇█▇▇▆▇</td></tr><tr><td>test_loss</td><td>█▇▆▄▃▂▂▂▂▂▂▂▁▁▂▁▁▁▂▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄█████████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▆▄▃▃▃▂▂▂▂▁▂▃▂▂▂▃▂▁▂▂▂▁▁▂▂▃▂▂▁▁▂▂▁▁▂▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>98.82</td></tr><tr><td>test_loss</td><td>0.00026</td></tr><tr><td>train_accuracy</td><td>96.61167</td></tr><tr><td>train_loss</td><td>0.11866</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-fog-16</strong> at: <a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch/runs/32zht66j' target=\"_blank\">https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch/runs/32zht66j</a><br> View project at: <a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch' target=\"_blank\">https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260205_190534-32zht66j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(1, config.epochs + 1):\n",
    "    train(net, device, trainloader, optimizer, epoch)\n",
    "    test_accuracy = test(net, device, testloader)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
