{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d2e0ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "958dac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f943fd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drawn-galaxy-35</strong> at: <a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch/runs/kwoevc6c' target=\"_blank\">https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch/runs/kwoevc6c</a><br> View project at: <a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch' target=\"_blank\">https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch</a><br>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260207_012336-kwoevc6c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\SMAYA\\OneDrive\\Desktop\\python\\WARP-26-INSPECT\\wandb\\run-20260207_012516-6pd5hlpm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch/runs/6pd5hlpm' target=\"_blank\">soft-wildflower-36</a></strong> to <a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch' target=\"_blank\">https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch/runs/6pd5hlpm' target=\"_blank\">https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch/runs/6pd5hlpm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"CNN on MNIST pytorch\", save_code=True,\n",
    "           config={\n",
    "               \"batch_size\": 128,\n",
    "               \"epochs\": 20,\n",
    "               \"optimizer\": \"Adam\",\n",
    "               \"loss function\": \"CrossEntropyLoss\",\n",
    "           }\n",
    "           )\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "baf7325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "14930b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: torch.Size([60000, 28, 28])\n",
      "training labels shape: torch.Size([60000])\n",
      "testing data shape: torch.Size([10000, 28, 28])\n",
      "testing labels shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "#Prepare the MNIST dataset in this cell\n",
    "''\n",
    "train_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.RandomRotation(degrees=10),\n",
    "                                transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.7, 1.3)),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "''\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.1307,), (0.3081,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
    "print(f\"training data shape: {trainset.data.shape}\")\n",
    "print(f\"training labels shape: {trainset.targets.shape}\")\n",
    "print(f\"testing data shape: {testset.data.shape}\")\n",
    "print(f\"testing labels shape: {testset.targets.shape}\")\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c0e5b277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Neural_network(nn.Module):\\n    def __init__(self):\\n        super(Neural_network, self).__init__()\\n        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)#output dim = 14x14x8\\n        self.conv2 = nn.Conv2d(16, 28, 3, padding =1)#output dim = 7x7x16\\n        #self.dropout = nn.Dropout(p=0.15)\\n        self.normalization1 = nn.BatchNorm2d(16)\\n        self.normalization2 = nn.BatchNorm2d(28)\\n        self.normalization3 = nn.BatchNorm2d(32)\\n        self.conv3 = nn.Conv2d(28, 32, 3, padding =1)#output dim = 3x3x20\\n        #self.fc1 = nn.Linear(4*4*32, 16)\\n        self.fc2 = nn.Linear(32, 10)\\n        #self.fc3 = nn.Linear(16, 10)\\n        self.pool = nn.MaxPool2d(2, 2)\\n        self.gap = nn.AdaptiveAvgPool2d((1, 1))\\n    def forward(self,x):\\n        x= self.pool(F.leaky_relu(self.normalization1(self.conv1(x))))\\n        x= self.pool(F.leaky_relu(self.normalization2(self.conv2(x))))\\n        x= F.leaky_relu(self.normalization3(self.conv3(x)))\\n        #print(f\"Shape after conv3 and pool: {x.shape}\")\\n        x = self.gap(x)\\n        x= x.view(-1, 32)\\n        #x= F.leaky_relu(self.fc1(x))\\n        #x = self.dropout(x)\\n        x = self.fc2(x)\\n        #x = self.fc3(x)\\n        return x\\n\\nnet = Neural_network()\\nnet.to(device)\\n'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class Neural_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Neural_network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)#output dim = 14x14x8\n",
    "        self.conv2 = nn.Conv2d(16, 28, 3, padding =1)#output dim = 7x7x16\n",
    "        #self.dropout = nn.Dropout(p=0.15)\n",
    "        self.normalization1 = nn.BatchNorm2d(16)\n",
    "        self.normalization2 = nn.BatchNorm2d(28)\n",
    "        self.normalization3 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(28, 32, 3, padding =1)#output dim = 3x3x20\n",
    "        #self.fc1 = nn.Linear(4*4*32, 16)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        #self.fc3 = nn.Linear(16, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    def forward(self,x):\n",
    "        x= self.pool(F.leaky_relu(self.normalization1(self.conv1(x))))\n",
    "        x= self.pool(F.leaky_relu(self.normalization2(self.conv2(x))))\n",
    "        x= F.leaky_relu(self.normalization3(self.conv3(x)))\n",
    "        #print(f\"Shape after conv3 and pool: {x.shape}\")\n",
    "        x = self.gap(x)\n",
    "        x= x.view(-1, 32)\n",
    "        #x= F.leaky_relu(self.fc1(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Neural_network()\n",
    "net.to(device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7bdd27fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neural_network(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(16, 22, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv3): Conv2d(22, 14, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=14, out_features=21, bias=True)\n",
       "  (fc2): Linear(in_features=21, out_features=10, bias=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (normalization1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (normalization2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (normalization3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neural_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Neural_network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, padding=2)#output dim = 14x14x8\n",
    "        self.conv2 = nn.Conv2d(16, 22, 5, padding =2)#output dim = 7x7x16\n",
    "        self.conv3 = nn.Conv2d(22, 14, 5)#output dim = 3x3x20\n",
    "        self.fc1 = nn.Linear(14, 21)\n",
    "        self.fc2 = nn.Linear(21, 10)\n",
    "        #self.fc3 = nn.Linear(16, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.normalization1 = nn.BatchNorm2d(16)\n",
    "        self.normalization2 = nn.BatchNorm2d(22)\n",
    "        self.normalization3 = nn.BatchNorm2d(14)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    def forward(self,x):\n",
    "\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.normalization1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.normalization2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "\n",
    "        x = self.normalization3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.gap(x)\n",
    "        #print(f\"Shape after conv3 and pool: {x.shape}\")\n",
    "        x= x.view(-1, 14)# understand the math of this line\n",
    "        x= F.leaky_relu(self.fc1(x))\n",
    "\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        #x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Neural_network()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "725004cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the model: 17591\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(f\"Total number of parameters in the model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b865b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sched import scheduler\n",
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)#does net.parameters just make a vector of all the parameters in the model?\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "da1e64f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001CEA8E36580>\n"
     ]
    }
   ],
   "source": [
    "print(f\"{trainloader}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "75d7a084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nepochs = 20\\nfor epoch in range(epochs):\\n    dynamic_loss = 0.0\\n    for i,data in enumerate(trainloader):\\n        inputs, labels = data[0].to(device), data[1].to(device)\\n        optimizer.zero_grad()\\n        outputs = net(inputs)\\n        loss = loss_function(outputs, F.one_hot(labels, num_classes=10).float())\\n        loss.backward()\\n        optimizer.step()\\n        dynamic_loss += loss\\n        if i%200 ==99:\\n            print(f\"epoch {epoch+1}, batch {i+1}, loss: {dynamic_loss/400}\")\\n            dynamic_loss = 0.0 \\n'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#have to decide how to train the model in this cell ie epochs, batch size, backpropagation etc.\n",
    "'''\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    dynamic_loss = 0.0\n",
    "    for i,data in enumerate(trainloader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_function(outputs, F.one_hot(labels, num_classes=10).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        dynamic_loss += loss\n",
    "        if i%200 ==99:\n",
    "            print(f\"epoch {epoch+1}, batch {i+1}, loss: {dynamic_loss/400}\")\n",
    "            dynamic_loss = 0.0 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f8780841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom tqdm import tqdm\\npbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}\")\\nfor batch_idx, (data,target) in enumerate(pbar):\\n    inputs, labels = data[0].to(device), data[1].to(device)\\n    optimizer.zero_grad()\\n    outputs = net(inputs)\\n    loss = loss_function(outputs, F.one_hot(labels, num_classes=10).float())\\n    loss.backward()\\n    optimizer.step()\\n    pbar.set_postfix({\\'loss\\': loss.item()})\\n    wandb.log({\"train_loss\": loss.item()})\\n    dynamic_loss += loss\\n    if i%200 ==99:\\n        print(f\"epoch {epoch+1}, batch {i+1}, loss: {dynamic_loss/400}\")\\n        dynamic_loss = 0.0 \\n'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from tqdm import tqdm\n",
    "pbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}\")\n",
    "for batch_idx, (data,target) in enumerate(pbar):\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = loss_function(outputs, F.one_hot(labels, num_classes=10).float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_postfix({'loss': loss.item()})\n",
    "    wandb.log({\"train_loss\": loss.item()})\n",
    "    dynamic_loss += loss\n",
    "    if i%200 ==99:\n",
    "        print(f\"epoch {epoch+1}, batch {i+1}, loss: {dynamic_loss/400}\")\n",
    "        dynamic_loss = 0.0 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e1077baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, device, train_loader, optimizer, epoch):\n",
    "    net.train()\n",
    "    # tqdm creates the progress bar\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    train_correct =0\n",
    "    train_total =0\n",
    "    for i, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(data)\n",
    "        loss = loss_function(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, train_predicted = torch.max(outputs, 1)\n",
    "        train_total += len(target)\n",
    "        train_correct += (train_predicted == target).sum().item()\n",
    "        # Update the bar with the latest loss\n",
    "        pbar.set_description(desc=f'Epoch {epoch} Loss={loss.item():.4f}')\n",
    "        \n",
    "        # Log training loss to wandb\n",
    "        wandb.log({\"train_loss\": loss.item(), \"train_accuracy\": 100.0 * train_correct / train_total})\n",
    "        pbar.set_postfix({'loss': loss.item(), 'train_accuracy': 100.0 * train_correct / train_total})\n",
    "\n",
    "def test(net, device, testloader):\n",
    "    net.eval()\n",
    "    correct_labels = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            test_loss += loss_function(outputs, labels).item()\n",
    "            #print(f\"outputs.data = {outputs.data}\")\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            #if (predicted == labels).sum().item():\n",
    "            correct_labels += (predicted == labels).sum().item()\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    accuracy = 100.0 * correct_labels / len(testloader.dataset)\n",
    "    wandb.log({\"test_accuracy\": accuracy, \"test_loss\": test_loss})\n",
    "    print(f\"\\n Test Set: Accuracy: {accuracy:.4f}%, test_loss: {test_loss} \\n\")\n",
    "# Log test results to wandb\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d71c37e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'correct_labels = 0\\ntotal_labels = 0\\nwith torch.no_grad():\\n    for data in testloader:\\n        images, labels = data[0].to(device), data[1].to(device)\\n        outputs = net(images)\\n        probs = F.softmax(outputs, dim =1)\\n        #print(f\"outputs.data = {outputs.data}\")\\n        _, predicted = torch.max(probs, 1)\\n        total_labels += labels.size(0)\\n        #if (predicted == labels).sum().item():\\n        correct_labels += (predicted == labels).sum().item()\\n        wandb.log({\"test_accuracy\": 100*correct_labels/total_labels})\\nprint(f\"accuracy of the network on the 10000 test images: {100*correct_labels/total_labels} %\")\\n'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''correct_labels = 0\n",
    "total_labels = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        probs = F.softmax(outputs, dim =1)\n",
    "        #print(f\"outputs.data = {outputs.data}\")\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "        total_labels += labels.size(0)\n",
    "        #if (predicted == labels).sum().item():\n",
    "        correct_labels += (predicted == labels).sum().item()\n",
    "        wandb.log({\"test_accuracy\": 100*correct_labels/total_labels})\n",
    "print(f\"accuracy of the network on the 10000 test images: {100*correct_labels/total_labels} %\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2d90527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss=0.2000: 100%|██████████| 938/938 [00:56<00:00, 16.68it/s, loss=0.2, train_accuracy=82.6]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 96.8700%, test_loss: 0.0014463310131337494 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss=0.0960: 100%|██████████| 938/938 [03:11<00:00,  4.89it/s, loss=0.096, train_accuracy=94.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.2700%, test_loss: 0.0008634206339833327 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss=0.1380: 100%|██████████| 938/938 [03:01<00:00,  5.18it/s, loss=0.138, train_accuracy=95.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.4900%, test_loss: 0.0006326577275758609 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss=0.0938: 100%|██████████| 938/938 [02:30<00:00,  6.23it/s, loss=0.0938, train_accuracy=96.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.1200%, test_loss: 0.00045174537799757674 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss=0.0095: 100%|██████████| 938/938 [00:53<00:00, 17.68it/s, loss=0.00955, train_accuracy=96.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.7500%, test_loss: 0.0006040767942045932 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss=0.5927: 100%|██████████| 938/938 [00:57<00:00, 16.19it/s, loss=0.593, train_accuracy=96.8]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 98.8700%, test_loss: 0.0005023865458380896 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss=0.3533: 100%|██████████| 938/938 [00:55<00:00, 16.80it/s, loss=0.353, train_accuracy=97]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.2000%, test_loss: 0.00039189110576371604 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss=0.0890: 100%|██████████| 938/938 [01:36<00:00,  9.76it/s, loss=0.089, train_accuracy=97]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.1200%, test_loss: 0.0003985341312327364 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss=0.2348: 100%|██████████| 938/938 [00:55<00:00, 17.04it/s, loss=0.235, train_accuracy=97.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.0700%, test_loss: 0.0003743603291002728 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss=0.0108: 100%|██████████| 938/938 [01:53<00:00,  8.30it/s, loss=0.0108, train_accuracy=97.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.0700%, test_loss: 0.0004172200258482917 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss=0.0722: 100%|██████████| 938/938 [01:39<00:00,  9.45it/s, loss=0.0722, train_accuracy=97.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.2000%, test_loss: 0.00035913770653132817 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss=0.0619: 100%|██████████| 938/938 [01:57<00:00,  8.00it/s, loss=0.0619, train_accuracy=97.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.0800%, test_loss: 0.00039583181726557084 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss=0.1266: 100%|██████████| 938/938 [02:15<00:00,  6.95it/s, loss=0.127, train_accuracy=97.6]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.1700%, test_loss: 0.0003858005427979151 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss=0.0267: 100%|██████████| 938/938 [02:06<00:00,  7.42it/s, loss=0.0267, train_accuracy=97.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.3400%, test_loss: 0.0002928973363988916 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss=0.0288: 100%|██████████| 938/938 [02:15<00:00,  6.92it/s, loss=0.0288, train_accuracy=97.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.2200%, test_loss: 0.0003810078739437813 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss=0.0795: 100%|██████████| 938/938 [02:12<00:00,  7.08it/s, loss=0.0795, train_accuracy=97.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.2600%, test_loss: 0.0003320362321619541 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss=0.0085: 100%|██████████| 938/938 [02:02<00:00,  7.63it/s, loss=0.00848, train_accuracy=97.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.3200%, test_loss: 0.0003351334675156977 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss=0.0266: 100%|██████████| 938/938 [02:15<00:00,  6.92it/s, loss=0.0266, train_accuracy=97.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.2900%, test_loss: 0.00030983767765701485 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss=0.0861: 100%|██████████| 938/938 [01:47<00:00,  8.71it/s, loss=0.0861, train_accuracy=97.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.2500%, test_loss: 0.000347923086947776 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss=0.0289: 100%|██████████| 938/938 [01:13<00:00, 12.81it/s, loss=0.0289, train_accuracy=97.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set: Accuracy: 99.2200%, test_loss: 0.000325434441706966 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁▅▆▇▆▇█▇▇▇█▇████████</td></tr><tr><td>test_loss</td><td>█▄▃▂▃▂▂▂▁▂▁▂▂▁▂▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▄▇▇▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>train_loss</td><td>██▂▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▂▁▂▁▁▁▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>99.22</td></tr><tr><td>test_loss</td><td>0.00033</td></tr><tr><td>train_accuracy</td><td>97.86333</td></tr><tr><td>train_loss</td><td>0.02893</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-wildflower-36</strong> at: <a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch/runs/6pd5hlpm' target=\"_blank\">https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch/runs/6pd5hlpm</a><br> View project at: <a href='https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch' target=\"_blank\">https://wandb.ai/b25cs1065-indian-institute-of-technology-jodhpur/CNN%20on%20MNIST%20pytorch</a><br>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260207_012516-6pd5hlpm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(1, config.epochs + 1):\n",
    "    train(net, device, trainloader, optimizer, epoch)\n",
    "    test_accuracy = test(net, device, testloader)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
